{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.368721500Z",
     "start_time": "2024-11-14T06:05:17.590328500Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pyBeamSim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from model.model_with_attention import Encoder_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      drift1_len  quad1_len  quad1_gra  drift2_len  quad2_len  quad2_gra  \\\n0       0.196427   0.182149   8.681705    0.250776   0.373205 -10.382813   \n1       0.403793   0.429330   9.532400    0.329042   0.112563   5.724299   \n2       0.329227   0.152616  17.639344    0.157597   0.423441 -10.074239   \n3       0.301287   0.423994   4.371640    0.204886   0.154771  15.747145   \n4       0.349911   0.361642  -5.058993    0.480674   0.409044  15.117262   \n...          ...        ...        ...         ...        ...        ...   \n9995    0.396447   0.382212   5.604220    0.354495   0.222928 -15.905318   \n9996    0.114760   0.346819  -6.901604    0.369893   0.397082  -7.834693   \n9997    0.490315   0.249131  -1.215864    0.268813   0.141644 -13.663927   \n9998    0.342286   0.275773   3.625842    0.436554   0.132755  -1.138420   \n9999    0.483956   0.459528 -16.066765    0.282521   0.376859  -1.417207   \n\n            x_avg0    x_avg1        x_avg2    x_avg3    x_avg4  \n0    -2.582750e-09  0.000022 -5.354477e-07 -0.000005  0.000004  \n1    -2.582750e-09  0.000042  5.218342e-07  0.000214 -0.000167  \n2    -2.582750e-09  0.000036 -1.753044e-06  0.000002  0.000137  \n3    -2.582750e-09  0.000034  1.601237e-07  0.000001 -0.000005  \n4    -2.582750e-09  0.000039 -2.395925e-06 -0.000260  0.000038  \n...            ...       ...           ...       ...       ...  \n9995 -2.582750e-09  0.000043  1.511706e-07 -0.000011  0.000103  \n9996 -2.582750e-09  0.000013 -2.018009e-06 -0.000080  0.000078  \n9997 -2.582750e-09  0.000044 -2.898933e-07 -0.000019 -0.000071  \n9998 -2.582750e-09  0.000038 -1.473266e-06 -0.000029 -0.000023  \n9999 -2.582750e-09  0.000044  5.615202e-07  0.000529 -0.000772  \n\n[10000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>drift1_len</th>\n      <th>quad1_len</th>\n      <th>quad1_gra</th>\n      <th>drift2_len</th>\n      <th>quad2_len</th>\n      <th>quad2_gra</th>\n      <th>x_avg0</th>\n      <th>x_avg1</th>\n      <th>x_avg2</th>\n      <th>x_avg3</th>\n      <th>x_avg4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.196427</td>\n      <td>0.182149</td>\n      <td>8.681705</td>\n      <td>0.250776</td>\n      <td>0.373205</td>\n      <td>-10.382813</td>\n      <td>-2.582750e-09</td>\n      <td>0.000022</td>\n      <td>-5.354477e-07</td>\n      <td>-0.000005</td>\n      <td>0.000004</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.403793</td>\n      <td>0.429330</td>\n      <td>9.532400</td>\n      <td>0.329042</td>\n      <td>0.112563</td>\n      <td>5.724299</td>\n      <td>-2.582750e-09</td>\n      <td>0.000042</td>\n      <td>5.218342e-07</td>\n      <td>0.000214</td>\n      <td>-0.000167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.329227</td>\n      <td>0.152616</td>\n      <td>17.639344</td>\n      <td>0.157597</td>\n      <td>0.423441</td>\n      <td>-10.074239</td>\n      <td>-2.582750e-09</td>\n      <td>0.000036</td>\n      <td>-1.753044e-06</td>\n      <td>0.000002</td>\n      <td>0.000137</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.301287</td>\n      <td>0.423994</td>\n      <td>4.371640</td>\n      <td>0.204886</td>\n      <td>0.154771</td>\n      <td>15.747145</td>\n      <td>-2.582750e-09</td>\n      <td>0.000034</td>\n      <td>1.601237e-07</td>\n      <td>0.000001</td>\n      <td>-0.000005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.349911</td>\n      <td>0.361642</td>\n      <td>-5.058993</td>\n      <td>0.480674</td>\n      <td>0.409044</td>\n      <td>15.117262</td>\n      <td>-2.582750e-09</td>\n      <td>0.000039</td>\n      <td>-2.395925e-06</td>\n      <td>-0.000260</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.396447</td>\n      <td>0.382212</td>\n      <td>5.604220</td>\n      <td>0.354495</td>\n      <td>0.222928</td>\n      <td>-15.905318</td>\n      <td>-2.582750e-09</td>\n      <td>0.000043</td>\n      <td>1.511706e-07</td>\n      <td>-0.000011</td>\n      <td>0.000103</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.114760</td>\n      <td>0.346819</td>\n      <td>-6.901604</td>\n      <td>0.369893</td>\n      <td>0.397082</td>\n      <td>-7.834693</td>\n      <td>-2.582750e-09</td>\n      <td>0.000013</td>\n      <td>-2.018009e-06</td>\n      <td>-0.000080</td>\n      <td>0.000078</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.490315</td>\n      <td>0.249131</td>\n      <td>-1.215864</td>\n      <td>0.268813</td>\n      <td>0.141644</td>\n      <td>-13.663927</td>\n      <td>-2.582750e-09</td>\n      <td>0.000044</td>\n      <td>-2.898933e-07</td>\n      <td>-0.000019</td>\n      <td>-0.000071</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.342286</td>\n      <td>0.275773</td>\n      <td>3.625842</td>\n      <td>0.436554</td>\n      <td>0.132755</td>\n      <td>-1.138420</td>\n      <td>-2.582750e-09</td>\n      <td>0.000038</td>\n      <td>-1.473266e-06</td>\n      <td>-0.000029</td>\n      <td>-0.000023</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.483956</td>\n      <td>0.459528</td>\n      <td>-16.066765</td>\n      <td>0.282521</td>\n      <td>0.376859</td>\n      <td>-1.417207</td>\n      <td>-2.582750e-09</td>\n      <td>0.000044</td>\n      <td>5.615202e-07</td>\n      <td>0.000529</td>\n      <td>-0.000772</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data_50000.csv', index_col=0)\n",
    "df = df[:10000]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.515251700Z",
     "start_time": "2024-11-14T06:05:21.369721300Z"
    }
   },
   "id": "5c3c00b43e88ee7a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "      drift1_len  quad1_len  quad1_gra  drift2_len  quad2_len  quad2_gra  \\\n0       0.196427   0.182149   8.681705    0.250776   0.373205 -10.382813   \n1       0.403793   0.429330   9.532400    0.329042   0.112563   5.724299   \n2       0.329227   0.152616  17.639344    0.157597   0.423441 -10.074239   \n3       0.301287   0.423994   4.371640    0.204886   0.154771  15.747145   \n4       0.349911   0.361642  -5.058993    0.480674   0.409044  15.117262   \n...          ...        ...        ...         ...        ...        ...   \n9995    0.396447   0.382212   5.604220    0.354495   0.222928 -15.905318   \n9996    0.114760   0.346819  -6.901604    0.369893   0.397082  -7.834693   \n9997    0.490315   0.249131  -1.215864    0.268813   0.141644 -13.663927   \n9998    0.342286   0.275773   3.625842    0.436554   0.132755  -1.138420   \n9999    0.483956   0.459528 -16.066765    0.282521   0.376859  -1.417207   \n\n            x_avg0    x_avg1    x_avg2    x_avg3    x_avg4  \n0    -2.582750e-09 -0.916464  0.037908  0.325505  0.351625  \n1    -2.582750e-09  0.974384  0.519220  1.203486 -0.791223  \n2    -2.582750e-09  0.406076 -0.516384  0.351111  1.238314  \n3    -2.582750e-09  0.181292  0.354557  0.348436  0.293746  \n4    -2.582750e-09  0.665285 -0.809047 -0.698823  0.578823  \n...            ...       ...       ...       ...       ...  \n9995 -2.582750e-09  1.005996  0.350481  0.299260  1.014616  \n9996 -2.582750e-09 -1.790811 -0.637006  0.021067  0.848611  \n9997 -2.582750e-09  1.149614  0.149693  0.267634 -0.147845  \n9998 -2.582750e-09  0.590942 -0.389020  0.225882  0.168832  \n9999 -2.582750e-09  1.151782  0.537287  2.466611 -4.836316  \n\n[10000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>drift1_len</th>\n      <th>quad1_len</th>\n      <th>quad1_gra</th>\n      <th>drift2_len</th>\n      <th>quad2_len</th>\n      <th>quad2_gra</th>\n      <th>x_avg0</th>\n      <th>x_avg1</th>\n      <th>x_avg2</th>\n      <th>x_avg3</th>\n      <th>x_avg4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.196427</td>\n      <td>0.182149</td>\n      <td>8.681705</td>\n      <td>0.250776</td>\n      <td>0.373205</td>\n      <td>-10.382813</td>\n      <td>-2.582750e-09</td>\n      <td>-0.916464</td>\n      <td>0.037908</td>\n      <td>0.325505</td>\n      <td>0.351625</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.403793</td>\n      <td>0.429330</td>\n      <td>9.532400</td>\n      <td>0.329042</td>\n      <td>0.112563</td>\n      <td>5.724299</td>\n      <td>-2.582750e-09</td>\n      <td>0.974384</td>\n      <td>0.519220</td>\n      <td>1.203486</td>\n      <td>-0.791223</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.329227</td>\n      <td>0.152616</td>\n      <td>17.639344</td>\n      <td>0.157597</td>\n      <td>0.423441</td>\n      <td>-10.074239</td>\n      <td>-2.582750e-09</td>\n      <td>0.406076</td>\n      <td>-0.516384</td>\n      <td>0.351111</td>\n      <td>1.238314</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.301287</td>\n      <td>0.423994</td>\n      <td>4.371640</td>\n      <td>0.204886</td>\n      <td>0.154771</td>\n      <td>15.747145</td>\n      <td>-2.582750e-09</td>\n      <td>0.181292</td>\n      <td>0.354557</td>\n      <td>0.348436</td>\n      <td>0.293746</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.349911</td>\n      <td>0.361642</td>\n      <td>-5.058993</td>\n      <td>0.480674</td>\n      <td>0.409044</td>\n      <td>15.117262</td>\n      <td>-2.582750e-09</td>\n      <td>0.665285</td>\n      <td>-0.809047</td>\n      <td>-0.698823</td>\n      <td>0.578823</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.396447</td>\n      <td>0.382212</td>\n      <td>5.604220</td>\n      <td>0.354495</td>\n      <td>0.222928</td>\n      <td>-15.905318</td>\n      <td>-2.582750e-09</td>\n      <td>1.005996</td>\n      <td>0.350481</td>\n      <td>0.299260</td>\n      <td>1.014616</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.114760</td>\n      <td>0.346819</td>\n      <td>-6.901604</td>\n      <td>0.369893</td>\n      <td>0.397082</td>\n      <td>-7.834693</td>\n      <td>-2.582750e-09</td>\n      <td>-1.790811</td>\n      <td>-0.637006</td>\n      <td>0.021067</td>\n      <td>0.848611</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.490315</td>\n      <td>0.249131</td>\n      <td>-1.215864</td>\n      <td>0.268813</td>\n      <td>0.141644</td>\n      <td>-13.663927</td>\n      <td>-2.582750e-09</td>\n      <td>1.149614</td>\n      <td>0.149693</td>\n      <td>0.267634</td>\n      <td>-0.147845</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.342286</td>\n      <td>0.275773</td>\n      <td>3.625842</td>\n      <td>0.436554</td>\n      <td>0.132755</td>\n      <td>-1.138420</td>\n      <td>-2.582750e-09</td>\n      <td>0.590942</td>\n      <td>-0.389020</td>\n      <td>0.225882</td>\n      <td>0.168832</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.483956</td>\n      <td>0.459528</td>\n      <td>-16.066765</td>\n      <td>0.282521</td>\n      <td>0.376859</td>\n      <td>-1.417207</td>\n      <td>-2.582750e-09</td>\n      <td>1.151782</td>\n      <td>0.537287</td>\n      <td>2.466611</td>\n      <td>-4.836316</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler1.fit_transform(df)\n",
    "epsilon = 1e-10\n",
    "\n",
    "change_columns = df.columns[-4:].tolist()\n",
    "# change_columns.remove('x_avg0')\n",
    "df[change_columns] = scaler2.fit_transform(df[change_columns])\n",
    "df[change_columns] = np.where(df[change_columns] == 0, epsilon, df[change_columns])\n",
    "# df['x_avg0'] = 0\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.582780200Z",
     "start_time": "2024-11-14T06:05:21.510250500Z"
    }
   },
   "id": "edfd93bbae280a74"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: torch.Size([8000, 5, 1])\n",
      "X_train.shape: torch.Size([8000, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "use_models = ['GRU', 'LSTM']\n",
    "\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0\n",
    "weight_decay = 0\n",
    "teacher_forcing = 0.5\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "use_model = use_models[0]\n",
    "\n",
    "# last 5 columns as y_true\n",
    "X = df.iloc[:, :-5]\n",
    "y = df.iloc[:, -5:]\n",
    "# transform df into tensor\n",
    "X = torch.Tensor(X.values).unsqueeze(2) # add a dim: input_size\n",
    "y = torch.Tensor(y.values).unsqueeze(2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'X_train.shape: {X_train.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.675306300Z",
     "start_time": "2024-11-14T06:05:21.558783900Z"
    }
   },
   "id": "2e77b8feaf55c4ff"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first y:  tensor([[-2.5827e-09],\n",
      "        [-6.6649e-01],\n",
      "        [-2.2531e+00],\n",
      "        [-1.0563e-01],\n",
      "        [-5.2295e-01]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# generate dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "first_y = train_dataset[0][1]\n",
    "output_len = first_y.shape[0] - 1\n",
    "print('the first y: ', first_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.675306300Z",
     "start_time": "2024-11-14T06:05:21.605782600Z"
    }
   },
   "id": "98e91c3a607277b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4923b980ccd8594"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.98\n",
      "MAPE: 6.11%\n"
     ]
    }
   ],
   "source": [
    "# use mean absolute percent error as criterion\n",
    "# get mean mape for each batch\n",
    "def get_mape(tensor_true, tensor_pred):\n",
    "    \"\"\"\n",
    "    :param tensor_true : (batch_size, output_size) or (output_size,)\n",
    "    :param tensor_pred : same shape as tensor_true\n",
    "    \"\"\"\n",
    "\n",
    "    mape = torch.abs((tensor_pred - tensor_true) / tensor_true) * 100\n",
    "\n",
    "    if tensor_true.dim() == 2:\n",
    "        return torch.mean(mape, dim=1).mean()  \n",
    "    else:\n",
    "        return torch.mean(mape)  \n",
    "# and R2 score\n",
    "\n",
    "def compute_column_mape(tensor_true, tensor_pred):\n",
    "    mape = torch.abs((tensor_pred - tensor_true) / tensor_true) * 100\n",
    "    mape_per_column = torch.mean(mape, dim=0)  # for column\n",
    "    return mape_per_column\n",
    "\n",
    "def count_all_parameters(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\n",
    "y_true = torch.Tensor([100, 200, 300])\n",
    "y_pred = torch.Tensor([110, 190, 310])\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mape = get_mape(y_true, y_pred)\n",
    "print(f'R^2 Score: {r2:.2f}')\n",
    "print(f'MAPE: {mape:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.676309100Z",
     "start_time": "2024-11-14T06:05:21.620783900Z"
    }
   },
   "id": "88c5af515a7e9762"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(model, dataLoader, test_loader, optimizer, criterion, device, num_epochs, clip=1):\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in dataLoader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X, y, use_tf=True)\n",
    "            # remove the first avg_0(the input)\n",
    "            loss = criterion(y_pred, y[:, 1:, :])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() # for a batch\n",
    "        \n",
    "        avg_train_loss = total_loss / len(dataLoader)   # len(dataloader): nums of batch\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  \n",
    "            for X_val, y_val in test_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                \n",
    "                y_val_pred = model(X_val, y_val)\n",
    "                val_loss = criterion(y_val_pred, y_val[:, 1:, :])\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        test_losses.append(avg_val_loss)\n",
    "        \n",
    "        # scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: train_loss: {avg_train_loss:.10f}, valid_loss: {avg_val_loss:.10f}')\n",
    "    return train_losses, test_losses\n",
    "        \n",
    "def evaluate(model, dataLoader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataLoader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_pred = model(X, y)\n",
    "            loss = criterion(y[:, 1:, :], y_pred)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def evaluate_test_mape_gpu(model, test_loader):\n",
    "    model.eval()\n",
    "    total_num = 0\n",
    "    total_mape = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            y_pred = model(X, y)\n",
    "            mape = get_mape(y[:, 1:, :], y_pred)\n",
    "            total_mape += mape\n",
    "            total_num += X.shape[0]\n",
    "    return total_mape / len(test_loader)\n",
    "\n",
    "def evaluate_test_mape_gpu_column(model, test_loader):\n",
    "    model.eval()\n",
    "    total_mape = None  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            y_pred = model(X, y)\n",
    "\n",
    "            mape = compute_column_mape(y[:, 1:, :], y_pred)  \n",
    "            if total_mape is None:\n",
    "                total_mape = mape\n",
    "            else:\n",
    "                total_mape += mape\n",
    "    \n",
    "    total_mape = total_mape.squeeze().tolist()\n",
    "    column_mape = [round(per_mape / len(test_loader), 2) for per_mape in total_mape]\n",
    "    return column_mape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:21.676309100Z",
     "start_time": "2024-11-14T06:05:21.643791800Z"
    }
   },
   "id": "2dadb0ec618c8eb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a96b98792a9646e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 191617 parameters\n"
     ]
    }
   ],
   "source": [
    "model = Encoder_Decoder(input_size=input_size,\n",
    "                        encoder_hidden_size=hidden_size,\n",
    "                        decoder_hidden_size=hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        model_device=device,\n",
    "                        num_layers=num_layers,\n",
    "                        teacher_forcing=teacher_forcing,\n",
    "                        dropout=dropout)\n",
    "\n",
    "model.to(device)\n",
    "print(f'model has {count_all_parameters(model)} parameters')\n",
    "# model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:05:24.976139800Z",
     "start_time": "2024-11-14T06:05:21.664309200Z"
    }
   },
   "id": "f72453cee83586ff"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.9776477370, valid_loss: 0.9915959984\n",
      "Epoch 2: train_loss: 0.9366370344, valid_loss: 0.9201309532\n",
      "Epoch 3: train_loss: 0.6897199731, valid_loss: 0.6529634530\n",
      "Epoch 4: train_loss: 0.5711501818, valid_loss: 0.6155170519\n",
      "Epoch 5: train_loss: 0.5129190326, valid_loss: 0.5076123867\n",
      "Epoch 6: train_loss: 0.4676607955, valid_loss: 0.4952647947\n",
      "Epoch 7: train_loss: 0.4388744477, valid_loss: 0.5232056775\n",
      "Epoch 8: train_loss: 0.4331955042, valid_loss: 0.4336161716\n",
      "Epoch 9: train_loss: 0.3840420827, valid_loss: 0.4076882638\n",
      "Epoch 10: train_loss: 0.3747138909, valid_loss: 0.3755833241\n",
      "Epoch 11: train_loss: 0.3534379152, valid_loss: 0.3848207705\n",
      "Epoch 12: train_loss: 0.3334836854, valid_loss: 0.4194956333\n",
      "Epoch 13: train_loss: 0.3223312169, valid_loss: 0.4023948731\n",
      "Epoch 14: train_loss: 0.3016265835, valid_loss: 0.3397919331\n",
      "Epoch 15: train_loss: 0.2962967891, valid_loss: 0.3192794067\n",
      "Epoch 16: train_loss: 0.2900438592, valid_loss: 0.3242895678\n",
      "Epoch 17: train_loss: 0.2740682635, valid_loss: 0.3040297315\n",
      "Epoch 18: train_loss: 0.2829689394, valid_loss: 0.3524383083\n",
      "Epoch 19: train_loss: 0.2721265177, valid_loss: 0.2986692642\n",
      "Epoch 20: train_loss: 0.2488764727, valid_loss: 0.2880405984\n",
      "Epoch 21: train_loss: 0.2480156664, valid_loss: 0.3223599847\n",
      "Epoch 22: train_loss: 0.2282848413, valid_loss: 0.2822052860\n",
      "Epoch 23: train_loss: 0.2341238686, valid_loss: 0.2816460729\n",
      "Epoch 24: train_loss: 0.2265881090, valid_loss: 0.2910324666\n",
      "Epoch 25: train_loss: 0.2189368518, valid_loss: 0.2851817631\n",
      "Epoch 26: train_loss: 0.2088260113, valid_loss: 0.2693413249\n",
      "Epoch 27: train_loss: 0.2165877131, valid_loss: 0.2429210180\n",
      "Epoch 28: train_loss: 0.2036911573, valid_loss: 0.3363796170\n",
      "Epoch 29: train_loss: 0.2040160306, valid_loss: 0.2393408301\n",
      "Epoch 30: train_loss: 0.1947050434, valid_loss: 0.2837600517\n",
      "Epoch 31: train_loss: 0.1903944901, valid_loss: 0.2459537997\n",
      "Epoch 32: train_loss: 0.1851996384, valid_loss: 0.2410733653\n",
      "Epoch 33: train_loss: 0.1941020944, valid_loss: 0.3023240417\n",
      "Epoch 34: train_loss: 0.1752066632, valid_loss: 0.2284497684\n",
      "Epoch 35: train_loss: 0.1817898940, valid_loss: 0.2545994637\n",
      "Epoch 36: train_loss: 0.1836942897, valid_loss: 0.2153668948\n",
      "Epoch 37: train_loss: 0.1692210925, valid_loss: 0.2443414973\n",
      "Epoch 38: train_loss: 0.1727462943, valid_loss: 0.2335663438\n",
      "Epoch 39: train_loss: 0.1688712674, valid_loss: 0.2467787513\n",
      "Epoch 40: train_loss: 0.1620333089, valid_loss: 0.2054782878\n",
      "Epoch 41: train_loss: 0.1558060530, valid_loss: 0.2014553668\n",
      "Epoch 42: train_loss: 0.1500114574, valid_loss: 0.2193143656\n",
      "Epoch 43: train_loss: 0.1546378292, valid_loss: 0.2165970611\n",
      "Epoch 44: train_loss: 0.1480990608, valid_loss: 0.2426304668\n",
      "Epoch 45: train_loss: 0.1510072898, valid_loss: 0.2155021951\n",
      "Epoch 46: train_loss: 0.1432507057, valid_loss: 0.2184227989\n",
      "Epoch 47: train_loss: 0.1434827133, valid_loss: 0.1992199388\n",
      "Epoch 48: train_loss: 0.1442735503, valid_loss: 0.2135906485\n",
      "Epoch 49: train_loss: 0.1416659738, valid_loss: 0.2242512978\n",
      "Epoch 50: train_loss: 0.1291027446, valid_loss: 0.2132532161\n",
      "Epoch 51: train_loss: 0.1313652022, valid_loss: 0.2303360975\n",
      "Epoch 52: train_loss: 0.1357240201, valid_loss: 0.2986326595\n",
      "Epoch 53: train_loss: 0.1339771162, valid_loss: 0.2418365013\n",
      "Epoch 54: train_loss: 0.1325053713, valid_loss: 0.2099255635\n",
      "Epoch 55: train_loss: 0.1257807910, valid_loss: 0.1941306002\n",
      "Epoch 56: train_loss: 0.1277984612, valid_loss: 0.2067949967\n",
      "Epoch 57: train_loss: 0.1205709285, valid_loss: 0.1869321496\n",
      "Epoch 58: train_loss: 0.1184231213, valid_loss: 0.2002196412\n",
      "Epoch 59: train_loss: 0.1173772165, valid_loss: 0.2159799805\n",
      "Epoch 60: train_loss: 0.1189168232, valid_loss: 0.2163448730\n",
      "Epoch 61: train_loss: 0.1215066819, valid_loss: 0.1930466001\n",
      "Epoch 62: train_loss: 0.1097862633, valid_loss: 0.2163212760\n",
      "Epoch 63: train_loss: 0.1146790645, valid_loss: 0.2326683071\n",
      "Epoch 64: train_loss: 0.1093102059, valid_loss: 0.1871852044\n",
      "Epoch 65: train_loss: 0.1067807603, valid_loss: 0.1822670538\n",
      "Epoch 66: train_loss: 0.1036532841, valid_loss: 0.1998449836\n",
      "Epoch 67: train_loss: 0.1064405791, valid_loss: 0.1843092209\n",
      "Epoch 68: train_loss: 0.1031996582, valid_loss: 0.2136819172\n",
      "Epoch 69: train_loss: 0.1006578388, valid_loss: 0.1981235999\n",
      "Epoch 70: train_loss: 0.1032148160, valid_loss: 0.2031459096\n",
      "Epoch 71: train_loss: 0.1044082369, valid_loss: 0.1832688982\n",
      "Epoch 72: train_loss: 0.0941914943, valid_loss: 0.1938512246\n",
      "Epoch 73: train_loss: 0.0941198123, valid_loss: 0.1993547250\n",
      "Epoch 74: train_loss: 0.0954319191, valid_loss: 0.1882125679\n",
      "Epoch 75: train_loss: 0.0885697064, valid_loss: 0.2331873230\n",
      "Epoch 76: train_loss: 0.0906501327, valid_loss: 0.1834949576\n",
      "Epoch 77: train_loss: 0.0903271143, valid_loss: 0.1958652481\n",
      "Epoch 78: train_loss: 0.0849957082, valid_loss: 0.1988851153\n",
      "Epoch 79: train_loss: 0.0872058116, valid_loss: 0.2158278679\n",
      "Epoch 80: train_loss: 0.0882363616, valid_loss: 0.2187563670\n",
      "Epoch 81: train_loss: 0.0807147447, valid_loss: 0.1892138375\n",
      "Epoch 82: train_loss: 0.0813668406, valid_loss: 0.1892967080\n",
      "Epoch 83: train_loss: 0.0854913556, valid_loss: 0.1907563347\n",
      "Epoch 84: train_loss: 0.0779872779, valid_loss: 0.2014830769\n",
      "Epoch 85: train_loss: 0.0756314660, valid_loss: 0.1841205200\n",
      "Epoch 86: train_loss: 0.0765544749, valid_loss: 0.1986455063\n",
      "Epoch 87: train_loss: 0.0775376467, valid_loss: 0.1920244789\n",
      "Epoch 88: train_loss: 0.0760053460, valid_loss: 0.2229851205\n",
      "Epoch 89: train_loss: 0.0779286662, valid_loss: 0.2136696409\n",
      "Epoch 90: train_loss: 0.0726735482, valid_loss: 0.1977203498\n",
      "Epoch 91: train_loss: 0.0694405012, valid_loss: 0.1902353251\n",
      "Epoch 92: train_loss: 0.0697196049, valid_loss: 0.1981583606\n",
      "Epoch 93: train_loss: 0.0694314832, valid_loss: 0.2199394885\n",
      "Epoch 94: train_loss: 0.0662996988, valid_loss: 0.2047313545\n",
      "Epoch 95: train_loss: 0.0668987698, valid_loss: 0.2067566551\n",
      "Epoch 96: train_loss: 0.0643509812, valid_loss: 0.1951979906\n",
      "Epoch 97: train_loss: 0.0648028582, valid_loss: 0.1923943881\n",
      "Epoch 98: train_loss: 0.0632550225, valid_loss: 0.1920837753\n",
      "Epoch 99: train_loss: 0.0636868672, valid_loss: 0.1974903825\n",
      "Epoch 100: train_loss: 0.0695692820, valid_loss: 0.1934194737\n",
      "Epoch 101: train_loss: 0.0637045459, valid_loss: 0.2091503823\n",
      "Epoch 102: train_loss: 0.0593504862, valid_loss: 0.1881958321\n",
      "Epoch 103: train_loss: 0.0593874486, valid_loss: 0.1940814501\n",
      "Epoch 104: train_loss: 0.0598147799, valid_loss: 0.2125376328\n",
      "Epoch 105: train_loss: 0.0613415844, valid_loss: 0.2014680363\n",
      "Epoch 106: train_loss: 0.0568930148, valid_loss: 0.2004974019\n",
      "Epoch 107: train_loss: 0.0531185121, valid_loss: 0.1940155947\n",
      "Epoch 108: train_loss: 0.0587285857, valid_loss: 0.2060776995\n",
      "Epoch 109: train_loss: 0.0545821611, valid_loss: 0.1973143388\n",
      "Epoch 110: train_loss: 0.0530819460, valid_loss: 0.1913730868\n",
      "Epoch 111: train_loss: 0.0518379922, valid_loss: 0.2001299609\n",
      "Epoch 112: train_loss: 0.0515825757, valid_loss: 0.1987961624\n",
      "Epoch 113: train_loss: 0.0511513197, valid_loss: 0.2001362925\n",
      "Epoch 114: train_loss: 0.0541662866, valid_loss: 0.2143307002\n",
      "Epoch 115: train_loss: 0.0465862798, valid_loss: 0.1885861466\n",
      "Epoch 116: train_loss: 0.0480788770, valid_loss: 0.1884277877\n",
      "Epoch 117: train_loss: 0.0519199164, valid_loss: 0.1997600659\n",
      "Epoch 118: train_loss: 0.0468413904, valid_loss: 0.2092892490\n",
      "Epoch 119: train_loss: 0.0440720619, valid_loss: 0.1926248835\n",
      "Epoch 120: train_loss: 0.0500636448, valid_loss: 0.2207425339\n",
      "Epoch 121: train_loss: 0.0449115003, valid_loss: 0.1988246231\n",
      "Epoch 122: train_loss: 0.0471774125, valid_loss: 0.2078019781\n",
      "Epoch 123: train_loss: 0.0425819781, valid_loss: 0.2291431832\n",
      "Epoch 124: train_loss: 0.0451661389, valid_loss: 0.2058028243\n",
      "Epoch 125: train_loss: 0.0419088187, valid_loss: 0.2228598250\n",
      "Epoch 126: train_loss: 0.0454041129, valid_loss: 0.1940798913\n",
      "Epoch 127: train_loss: 0.0409252223, valid_loss: 0.1999599007\n",
      "Epoch 128: train_loss: 0.0430067028, valid_loss: 0.2072335673\n",
      "Epoch 129: train_loss: 0.0386748672, valid_loss: 0.2002675282\n",
      "Epoch 130: train_loss: 0.0380650837, valid_loss: 0.2062965150\n",
      "Epoch 131: train_loss: 0.0393717098, valid_loss: 0.1964934180\n",
      "Epoch 132: train_loss: 0.0371525505, valid_loss: 0.1989316635\n",
      "Epoch 133: train_loss: 0.0355965300, valid_loss: 0.2087966385\n",
      "Epoch 134: train_loss: 0.0377847404, valid_loss: 0.1952194849\n",
      "Epoch 135: train_loss: 0.0389795302, valid_loss: 0.2202072232\n",
      "Epoch 136: train_loss: 0.0392976591, valid_loss: 0.2396321674\n",
      "Epoch 137: train_loss: 0.0399803063, valid_loss: 0.2027156232\n",
      "Epoch 138: train_loss: 0.0358498855, valid_loss: 0.2037348049\n",
      "Epoch 139: train_loss: 0.0331586113, valid_loss: 0.1997492472\n",
      "Epoch 140: train_loss: 0.0367132551, valid_loss: 0.2056703661\n",
      "Epoch 141: train_loss: 0.0385416130, valid_loss: 0.1987405508\n",
      "Epoch 142: train_loss: 0.0347678564, valid_loss: 0.2279544913\n",
      "Epoch 143: train_loss: 0.0372570580, valid_loss: 0.2010852979\n",
      "Epoch 144: train_loss: 0.0317195818, valid_loss: 0.1996218525\n",
      "Epoch 145: train_loss: 0.0308019815, valid_loss: 0.2024127408\n",
      "Epoch 146: train_loss: 0.0315065143, valid_loss: 0.2030385416\n",
      "Epoch 147: train_loss: 0.0314324042, valid_loss: 0.2028595863\n",
      "Epoch 148: train_loss: 0.0315823218, valid_loss: 0.2147803386\n",
      "Epoch 149: train_loss: 0.0318990463, valid_loss: 0.2229321462\n",
      "Epoch 150: train_loss: 0.0297764934, valid_loss: 0.2036813956\n",
      "Epoch 151: train_loss: 0.0295249005, valid_loss: 0.2107413374\n",
      "Epoch 152: train_loss: 0.0291453171, valid_loss: 0.2161207926\n",
      "Epoch 153: train_loss: 0.0316169304, valid_loss: 0.2080709627\n",
      "Epoch 154: train_loss: 0.0297033716, valid_loss: 0.2139385566\n",
      "Epoch 155: train_loss: 0.0287312993, valid_loss: 0.2047195290\n",
      "Epoch 156: train_loss: 0.0292364016, valid_loss: 0.2212643893\n",
      "Epoch 157: train_loss: 0.0291180955, valid_loss: 0.2195258136\n",
      "Epoch 158: train_loss: 0.0289790311, valid_loss: 0.2125119427\n",
      "Epoch 159: train_loss: 0.0306785470, valid_loss: 0.2199725760\n",
      "Epoch 160: train_loss: 0.0273796445, valid_loss: 0.2090750504\n",
      "Epoch 161: train_loss: 0.0281035947, valid_loss: 0.2180320355\n",
      "Epoch 162: train_loss: 0.0276896541, valid_loss: 0.2064098027\n",
      "Epoch 163: train_loss: 0.0241955011, valid_loss: 0.2125574797\n",
      "Epoch 164: train_loss: 0.0261476632, valid_loss: 0.2110993913\n",
      "Epoch 165: train_loss: 0.0261202672, valid_loss: 0.2287484757\n",
      "Epoch 166: train_loss: 0.0257197033, valid_loss: 0.2052927692\n",
      "Epoch 167: train_loss: 0.0236720921, valid_loss: 0.2141918666\n",
      "Epoch 168: train_loss: 0.0251981462, valid_loss: 0.2182131959\n",
      "Epoch 169: train_loss: 0.0230872931, valid_loss: 0.2161819041\n",
      "Epoch 170: train_loss: 0.0261153254, valid_loss: 0.2256748504\n",
      "Epoch 171: train_loss: 0.0213640004, valid_loss: 0.2174210199\n",
      "Epoch 172: train_loss: 0.0217282270, valid_loss: 0.2257615416\n",
      "Epoch 173: train_loss: 0.0230758019, valid_loss: 0.2129855265\n",
      "Epoch 174: train_loss: 0.0241436743, valid_loss: 0.2075360036\n",
      "Epoch 175: train_loss: 0.0252506471, valid_loss: 0.2165477173\n",
      "Epoch 176: train_loss: 0.0235168125, valid_loss: 0.2185641127\n",
      "Epoch 177: train_loss: 0.0220383679, valid_loss: 0.2109100190\n",
      "Epoch 178: train_loss: 0.0196323819, valid_loss: 0.2070815298\n",
      "Epoch 179: train_loss: 0.0196548739, valid_loss: 0.2135064784\n",
      "Epoch 180: train_loss: 0.0214901459, valid_loss: 0.2172338951\n",
      "Epoch 181: train_loss: 0.0203970828, valid_loss: 0.2187906606\n",
      "Epoch 182: train_loss: 0.0224194423, valid_loss: 0.2132259307\n",
      "Epoch 183: train_loss: 0.0200130107, valid_loss: 0.2118400102\n",
      "Epoch 184: train_loss: 0.0198095858, valid_loss: 0.2172111413\n",
      "Epoch 185: train_loss: 0.0196095906, valid_loss: 0.2169054965\n",
      "Epoch 186: train_loss: 0.0189398523, valid_loss: 0.2237806926\n",
      "Epoch 187: train_loss: 0.0240038965, valid_loss: 0.2344902107\n",
      "Epoch 188: train_loss: 0.0217627498, valid_loss: 0.2122014053\n",
      "Epoch 189: train_loss: 0.0176734895, valid_loss: 0.2173396079\n",
      "Epoch 190: train_loss: 0.0175963585, valid_loss: 0.2222440820\n",
      "Epoch 191: train_loss: 0.0178957897, valid_loss: 0.2150273959\n",
      "Epoch 192: train_loss: 0.0190311740, valid_loss: 0.2140501221\n",
      "Epoch 193: train_loss: 0.0196144673, valid_loss: 0.2124344013\n",
      "Epoch 194: train_loss: 0.0181217116, valid_loss: 0.2154545602\n",
      "Epoch 195: train_loss: 0.0174547155, valid_loss: 0.2146922247\n",
      "Epoch 196: train_loss: 0.0199050401, valid_loss: 0.2174662545\n",
      "Epoch 197: train_loss: 0.0189882447, valid_loss: 0.2368219546\n",
      "Epoch 198: train_loss: 0.0181779590, valid_loss: 0.2186889071\n",
      "Epoch 199: train_loss: 0.0176790061, valid_loss: 0.2197802006\n",
      "Epoch 200: train_loss: 0.0158400696, valid_loss: 0.2163676808\n",
      "Epoch 201: train_loss: 0.0188615470, valid_loss: 0.2252580167\n",
      "Epoch 202: train_loss: 0.0161656654, valid_loss: 0.2257460072\n",
      "Epoch 203: train_loss: 0.0167464720, valid_loss: 0.2127420292\n",
      "Epoch 204: train_loss: 0.0172298586, valid_loss: 0.2212574179\n",
      "Epoch 205: train_loss: 0.0162294122, valid_loss: 0.2166871489\n",
      "Epoch 206: train_loss: 0.0162890814, valid_loss: 0.2135336867\n",
      "Epoch 207: train_loss: 0.0146484221, valid_loss: 0.2285616463\n",
      "Epoch 208: train_loss: 0.0159029682, valid_loss: 0.2230945304\n",
      "Epoch 209: train_loss: 0.0160644009, valid_loss: 0.2231085007\n",
      "Epoch 210: train_loss: 0.0164715572, valid_loss: 0.2230689488\n",
      "Epoch 211: train_loss: 0.0160013449, valid_loss: 0.2160984855\n",
      "Epoch 212: train_loss: 0.0142574463, valid_loss: 0.2247843165\n",
      "Epoch 213: train_loss: 0.0185823300, valid_loss: 0.2184824380\n",
      "Epoch 214: train_loss: 0.0195590991, valid_loss: 0.2101051915\n",
      "Epoch 215: train_loss: 0.0155378306, valid_loss: 0.2209051470\n",
      "Epoch 216: train_loss: 0.0143564199, valid_loss: 0.2099533873\n",
      "Epoch 217: train_loss: 0.0141852006, valid_loss: 0.2222505165\n",
      "Epoch 218: train_loss: 0.0142086602, valid_loss: 0.2197289777\n",
      "Epoch 219: train_loss: 0.0161916964, valid_loss: 0.2336371830\n",
      "Epoch 220: train_loss: 0.0144410572, valid_loss: 0.2234166185\n",
      "Epoch 221: train_loss: 0.0131767179, valid_loss: 0.2151732454\n",
      "Epoch 222: train_loss: 0.0126783609, valid_loss: 0.2161477813\n",
      "Epoch 223: train_loss: 0.0158127052, valid_loss: 0.2166018849\n",
      "Epoch 224: train_loss: 0.0193467756, valid_loss: 0.2141315416\n",
      "Epoch 225: train_loss: 0.0150104940, valid_loss: 0.2167399013\n",
      "Epoch 226: train_loss: 0.0136414626, valid_loss: 0.2180461525\n",
      "Epoch 227: train_loss: 0.0126085404, valid_loss: 0.2143758903\n",
      "Epoch 228: train_loss: 0.0122582262, valid_loss: 0.2117161872\n",
      "Epoch 229: train_loss: 0.0123297850, valid_loss: 0.2126458627\n",
      "Epoch 230: train_loss: 0.0140753230, valid_loss: 0.2235010630\n",
      "Epoch 231: train_loss: 0.0137143945, valid_loss: 0.2461729287\n",
      "Epoch 232: train_loss: 0.0144922487, valid_loss: 0.2191780047\n",
      "Epoch 233: train_loss: 0.0128798540, valid_loss: 0.2312589139\n",
      "Epoch 234: train_loss: 0.0122377501, valid_loss: 0.2257698914\n",
      "Epoch 235: train_loss: 0.0131927815, valid_loss: 0.2249048343\n",
      "Epoch 236: train_loss: 0.0118735674, valid_loss: 0.2213507458\n",
      "Epoch 237: train_loss: 0.0131473192, valid_loss: 0.2217138167\n",
      "Epoch 238: train_loss: 0.0155960207, valid_loss: 0.2202339154\n",
      "Epoch 239: train_loss: 0.0126229069, valid_loss: 0.2136016316\n",
      "Epoch 240: train_loss: 0.0129305093, valid_loss: 0.2180376714\n",
      "Epoch 241: train_loss: 0.0112145564, valid_loss: 0.2179287812\n",
      "Epoch 242: train_loss: 0.0121612248, valid_loss: 0.2260414511\n",
      "Epoch 243: train_loss: 0.0111869454, valid_loss: 0.2271339167\n",
      "Epoch 244: train_loss: 0.0113909425, valid_loss: 0.2141783340\n",
      "Epoch 245: train_loss: 0.0112945935, valid_loss: 0.2243258120\n",
      "Epoch 246: train_loss: 0.0124917041, valid_loss: 0.2163005783\n",
      "Epoch 247: train_loss: 0.0124802919, valid_loss: 0.2314045755\n",
      "Epoch 248: train_loss: 0.0163521605, valid_loss: 0.2171430204\n",
      "Epoch 249: train_loss: 0.0136257227, valid_loss: 0.2197107021\n",
      "Epoch 250: train_loss: 0.0138767985, valid_loss: 0.2363521105\n",
      "Epoch 251: train_loss: 0.0139519426, valid_loss: 0.2164092790\n",
      "Epoch 252: train_loss: 0.0113158793, valid_loss: 0.2207483535\n",
      "Epoch 253: train_loss: 0.0112247671, valid_loss: 0.2173061380\n",
      "Epoch 254: train_loss: 0.0104118533, valid_loss: 0.2207766264\n",
      "Epoch 255: train_loss: 0.0093976436, valid_loss: 0.2179367063\n",
      "Epoch 256: train_loss: 0.0109524336, valid_loss: 0.2152741496\n",
      "Epoch 257: train_loss: 0.0101471823, valid_loss: 0.2293231995\n",
      "Epoch 258: train_loss: 0.0098464722, valid_loss: 0.2209889106\n",
      "Epoch 259: train_loss: 0.0096324274, valid_loss: 0.2230306743\n",
      "Epoch 260: train_loss: 0.0093994965, valid_loss: 0.2193761701\n",
      "Epoch 261: train_loss: 0.0103299797, valid_loss: 0.2194997342\n",
      "Epoch 262: train_loss: 0.0152205528, valid_loss: 0.2320262897\n",
      "Epoch 263: train_loss: 0.0145334018, valid_loss: 0.2194339475\n",
      "Epoch 264: train_loss: 0.0115628735, valid_loss: 0.2220466684\n",
      "Epoch 265: train_loss: 0.0144724771, valid_loss: 0.2189225862\n",
      "Epoch 266: train_loss: 0.0112867430, valid_loss: 0.2246600077\n",
      "Epoch 267: train_loss: 0.0143625523, valid_loss: 0.2205299158\n",
      "Epoch 268: train_loss: 0.0121500789, valid_loss: 0.2183386972\n",
      "Epoch 269: train_loss: 0.0100856835, valid_loss: 0.2254709357\n",
      "Epoch 270: train_loss: 0.0087873840, valid_loss: 0.2230680990\n",
      "Epoch 271: train_loss: 0.0078877647, valid_loss: 0.2174795251\n",
      "Epoch 272: train_loss: 0.0083181320, valid_loss: 0.2232237016\n",
      "Epoch 273: train_loss: 0.0078757490, valid_loss: 0.2132611042\n",
      "Epoch 274: train_loss: 0.0070590351, valid_loss: 0.2213468025\n",
      "Epoch 275: train_loss: 0.0084700584, valid_loss: 0.2280306700\n",
      "Epoch 276: train_loss: 0.0099838481, valid_loss: 0.2221684232\n",
      "Epoch 277: train_loss: 0.0123489899, valid_loss: 0.2360943691\n",
      "Epoch 278: train_loss: 0.0147765939, valid_loss: 0.2171296487\n",
      "Epoch 279: train_loss: 0.0153540804, valid_loss: 0.2248255811\n",
      "Epoch 280: train_loss: 0.0112211254, valid_loss: 0.2263046652\n",
      "Epoch 281: train_loss: 0.0104669253, valid_loss: 0.2228925140\n",
      "Epoch 282: train_loss: 0.0085040486, valid_loss: 0.2148539703\n",
      "Epoch 283: train_loss: 0.0097449499, valid_loss: 0.2162603131\n",
      "Epoch 284: train_loss: 0.0101467098, valid_loss: 0.2236274439\n",
      "Epoch 285: train_loss: 0.0090954306, valid_loss: 0.2170874556\n",
      "Epoch 286: train_loss: 0.0088642151, valid_loss: 0.2214291731\n",
      "Epoch 287: train_loss: 0.0076531384, valid_loss: 0.2223227639\n",
      "Epoch 288: train_loss: 0.0086208285, valid_loss: 0.2175587434\n",
      "Epoch 289: train_loss: 0.0081953612, valid_loss: 0.2218293063\n",
      "Epoch 290: train_loss: 0.0083330475, valid_loss: 0.2359441132\n",
      "Epoch 291: train_loss: 0.0083699618, valid_loss: 0.2201667754\n",
      "Epoch 292: train_loss: 0.0095495008, valid_loss: 0.2255495004\n",
      "Epoch 293: train_loss: 0.0105729633, valid_loss: 0.2292697323\n",
      "Epoch 294: train_loss: 0.0116296342, valid_loss: 0.2178430934\n",
      "Epoch 295: train_loss: 0.0094426717, valid_loss: 0.2239351384\n",
      "Epoch 296: train_loss: 0.0078288295, valid_loss: 0.2189209927\n",
      "Epoch 297: train_loss: 0.0067658349, valid_loss: 0.2294547698\n",
      "Epoch 298: train_loss: 0.0067183627, valid_loss: 0.2173524327\n",
      "Epoch 299: train_loss: 0.0087470272, valid_loss: 0.2236384726\n",
      "Epoch 300: train_loss: 0.0083829844, valid_loss: 0.2304102774\n",
      "Epoch 301: train_loss: 0.0149613412, valid_loss: 0.2248814241\n",
      "Epoch 302: train_loss: 0.0129688144, valid_loss: 0.2280349503\n",
      "Epoch 303: train_loss: 0.0099229416, valid_loss: 0.2188620972\n",
      "Epoch 304: train_loss: 0.0078175031, valid_loss: 0.2207965991\n",
      "Epoch 305: train_loss: 0.0075098010, valid_loss: 0.2225902607\n",
      "Epoch 306: train_loss: 0.0086609487, valid_loss: 0.2179846028\n",
      "Epoch 307: train_loss: 0.0085421792, valid_loss: 0.2264500218\n",
      "Epoch 308: train_loss: 0.0084279332, valid_loss: 0.2197172460\n",
      "Epoch 309: train_loss: 0.0104882125, valid_loss: 0.2263772059\n",
      "Epoch 310: train_loss: 0.0090172877, valid_loss: 0.2327301148\n",
      "Epoch 311: train_loss: 0.0086782084, valid_loss: 0.2231701817\n",
      "Epoch 312: train_loss: 0.0093111848, valid_loss: 0.2271086508\n",
      "Epoch 313: train_loss: 0.0079695375, valid_loss: 0.2174590467\n",
      "Epoch 314: train_loss: 0.0069373593, valid_loss: 0.2231978606\n",
      "Epoch 315: train_loss: 0.0063239052, valid_loss: 0.2203100976\n",
      "Epoch 316: train_loss: 0.0063980922, valid_loss: 0.2197789382\n",
      "Epoch 317: train_loss: 0.0059083313, valid_loss: 0.2210190562\n",
      "Epoch 318: train_loss: 0.0059895098, valid_loss: 0.2210740964\n",
      "Epoch 319: train_loss: 0.0077138576, valid_loss: 0.2202795614\n",
      "Epoch 320: train_loss: 0.0114810080, valid_loss: 0.2279909835\n",
      "Epoch 321: train_loss: 0.0104743574, valid_loss: 0.2360997396\n",
      "Epoch 322: train_loss: 0.0120808570, valid_loss: 0.2320499728\n",
      "Epoch 323: train_loss: 0.0135885684, valid_loss: 0.2386960848\n",
      "Epoch 324: train_loss: 0.0143410089, valid_loss: 0.2292293245\n",
      "Epoch 325: train_loss: 0.0111457878, valid_loss: 0.2330783508\n",
      "Epoch 326: train_loss: 0.0077056932, valid_loss: 0.2188232536\n",
      "Epoch 327: train_loss: 0.0060240383, valid_loss: 0.2205457096\n",
      "Epoch 328: train_loss: 0.0048338526, valid_loss: 0.2203210401\n",
      "Epoch 329: train_loss: 0.0053464361, valid_loss: 0.2179157808\n",
      "Epoch 330: train_loss: 0.0049521997, valid_loss: 0.2247381979\n",
      "Epoch 331: train_loss: 0.0049437398, valid_loss: 0.2256175852\n",
      "Epoch 332: train_loss: 0.0045073674, valid_loss: 0.2188527463\n",
      "Epoch 333: train_loss: 0.0064829827, valid_loss: 0.2199599324\n",
      "Epoch 334: train_loss: 0.0096291436, valid_loss: 0.2263005506\n",
      "Epoch 335: train_loss: 0.0088868967, valid_loss: 0.2271374431\n",
      "Epoch 336: train_loss: 0.0081571618, valid_loss: 0.2256036145\n",
      "Epoch 337: train_loss: 0.0110363071, valid_loss: 0.2280401574\n",
      "Epoch 338: train_loss: 0.0107296547, valid_loss: 0.2236298658\n",
      "Epoch 339: train_loss: 0.0112395978, valid_loss: 0.2271675942\n",
      "Epoch 340: train_loss: 0.0068626187, valid_loss: 0.2196789819\n",
      "Epoch 341: train_loss: 0.0060326573, valid_loss: 0.2212127508\n",
      "Epoch 342: train_loss: 0.0058987285, valid_loss: 0.2258512457\n",
      "Epoch 343: train_loss: 0.0062880205, valid_loss: 0.2207870497\n",
      "Epoch 344: train_loss: 0.0064557004, valid_loss: 0.2231280548\n",
      "Epoch 345: train_loss: 0.0083052005, valid_loss: 0.2312328685\n",
      "Epoch 346: train_loss: 0.0082363071, valid_loss: 0.2216310855\n",
      "Epoch 347: train_loss: 0.0075073355, valid_loss: 0.2262097099\n",
      "Epoch 348: train_loss: 0.0094068975, valid_loss: 0.2279126290\n",
      "Epoch 349: train_loss: 0.0085303690, valid_loss: 0.2271517003\n",
      "Epoch 350: train_loss: 0.0071234600, valid_loss: 0.2223866750\n",
      "Epoch 351: train_loss: 0.0058097100, valid_loss: 0.2229858898\n",
      "Epoch 352: train_loss: 0.0049087996, valid_loss: 0.2254011868\n",
      "Epoch 353: train_loss: 0.0045074877, valid_loss: 0.2206216683\n",
      "Epoch 354: train_loss: 0.0056952759, valid_loss: 0.2342232023\n",
      "Epoch 355: train_loss: 0.0061203841, valid_loss: 0.2221704568\n",
      "Epoch 356: train_loss: 0.0072832204, valid_loss: 0.2256971397\n",
      "Epoch 357: train_loss: 0.0131032927, valid_loss: 0.2286406839\n",
      "Epoch 358: train_loss: 0.0141705356, valid_loss: 0.2289949171\n",
      "Epoch 359: train_loss: 0.0124067834, valid_loss: 0.2207709285\n",
      "Epoch 360: train_loss: 0.0109790828, valid_loss: 0.2283588704\n",
      "Epoch 361: train_loss: 0.0114068551, valid_loss: 0.2245968925\n",
      "Epoch 362: train_loss: 0.0094630549, valid_loss: 0.2354808738\n",
      "Epoch 363: train_loss: 0.0097981676, valid_loss: 0.2212819625\n",
      "Epoch 364: train_loss: 0.0082706329, valid_loss: 0.2261917121\n",
      "Epoch 365: train_loss: 0.0052761759, valid_loss: 0.2186313113\n",
      "Epoch 366: train_loss: 0.0039821628, valid_loss: 0.2219646033\n",
      "Epoch 367: train_loss: 0.0041110830, valid_loss: 0.2257019999\n",
      "Epoch 368: train_loss: 0.0038285982, valid_loss: 0.2213981226\n",
      "Epoch 369: train_loss: 0.0040941839, valid_loss: 0.2205378944\n",
      "Epoch 370: train_loss: 0.0035670888, valid_loss: 0.2245687726\n",
      "Epoch 371: train_loss: 0.0046276799, valid_loss: 0.2212214149\n",
      "Epoch 372: train_loss: 0.0071409915, valid_loss: 0.2209058227\n",
      "Epoch 373: train_loss: 0.0057378863, valid_loss: 0.2236879338\n",
      "Epoch 374: train_loss: 0.0078752194, valid_loss: 0.2275881688\n",
      "Epoch 375: train_loss: 0.0105967678, valid_loss: 0.2408287129\n",
      "Epoch 376: train_loss: 0.0105366964, valid_loss: 0.2310161060\n",
      "Epoch 377: train_loss: 0.0092881659, valid_loss: 0.2278089966\n",
      "Epoch 378: train_loss: 0.0081966661, valid_loss: 0.2259589983\n",
      "Epoch 379: train_loss: 0.0059144913, valid_loss: 0.2205707124\n",
      "Epoch 380: train_loss: 0.0046136786, valid_loss: 0.2238512067\n",
      "Epoch 381: train_loss: 0.0046709099, valid_loss: 0.2213267353\n",
      "Epoch 382: train_loss: 0.0039048449, valid_loss: 0.2243552799\n",
      "Epoch 383: train_loss: 0.0038989100, valid_loss: 0.2227228503\n",
      "Epoch 384: train_loss: 0.0040881878, valid_loss: 0.2303321869\n",
      "Epoch 385: train_loss: 0.0046929884, valid_loss: 0.2242082348\n",
      "Epoch 386: train_loss: 0.0063936461, valid_loss: 0.2307530558\n",
      "Epoch 387: train_loss: 0.0087114666, valid_loss: 0.2302756300\n",
      "Epoch 388: train_loss: 0.0097143810, valid_loss: 0.2226308528\n",
      "Epoch 389: train_loss: 0.0135099361, valid_loss: 0.2273782305\n",
      "Epoch 390: train_loss: 0.0128362044, valid_loss: 0.2168771396\n",
      "Epoch 391: train_loss: 0.0069861109, valid_loss: 0.2213798338\n",
      "Epoch 392: train_loss: 0.0066057564, valid_loss: 0.2193807559\n",
      "Epoch 393: train_loss: 0.0050703967, valid_loss: 0.2269453267\n",
      "Epoch 394: train_loss: 0.0040717914, valid_loss: 0.2181882146\n",
      "Epoch 395: train_loss: 0.0036187421, valid_loss: 0.2286607977\n",
      "Epoch 396: train_loss: 0.0034712943, valid_loss: 0.2185459244\n",
      "Epoch 397: train_loss: 0.0040138757, valid_loss: 0.2196844192\n",
      "Epoch 398: train_loss: 0.0055464264, valid_loss: 0.2280745618\n",
      "Epoch 399: train_loss: 0.0064228950, valid_loss: 0.2322613543\n",
      "Epoch 400: train_loss: 0.0064408430, valid_loss: 0.2267216071\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses, test_losses = train(model, train_loader, test_loader, optimizer, criterion, device, num_epochs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:03.486902500Z",
     "start_time": "2024-11-14T06:05:24.978140100Z"
    }
   },
   "id": "77af650d99d8ffc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### save model and loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed25cbaf35960fa"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "save_pth = '../model_save'\n",
    "model_pth = 'seq2seq' + '_' + str(num_layers) + 'layers' +'.pth'\n",
    "torch.save(model.state_dict(), os.path.join(save_pth, model_pth))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:03.516454Z",
     "start_time": "2024-11-14T06:26:03.488913500Z"
    }
   },
   "id": "b700dbbb05073162"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "save_pth2 = '../record_for_plot'\n",
    "df_record = pd.DataFrame({\n",
    "    'epoch': range(1, num_epochs + 1),\n",
    "    'train_loss': train_losses,\n",
    "    'test_loss': test_losses\n",
    "})\n",
    "loss_pth = 'seq2seq' + '_' + str(num_layers) + 'layers' +'.csv'\n",
    "df_record.to_csv(os.path.join(save_pth2, loss_pth), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:03.532739400Z",
     "start_time": "2024-11-14T06:26:03.519472300Z"
    }
   },
   "id": "9ec4df94029118b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### result analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e59c8e7a0bdc159"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mape: 186.0795\n",
      "train_mape: 85.5267\n",
      "for each column \n",
      "test_column_mape: [37.79, 203.04, 157.05, 346.44] \n",
      "train_column_mape: [42.34, 178.95, 55.74, 65.07]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../model_save/seq2seq_2layers.pth'))\n",
    "\n",
    "# result analysis\n",
    "test_mape = evaluate_test_mape_gpu(model, test_loader)\n",
    "train_mape = evaluate_test_mape_gpu(model, train_loader)\n",
    "test_column_mape = evaluate_test_mape_gpu_column(model, test_loader)\n",
    "train_column_mape = evaluate_test_mape_gpu_column(model, train_loader)\n",
    "print(f'test_mape: {test_mape:.4f}')\n",
    "print(f'train_mape: {train_mape:.4f}')\n",
    "print(f'for each column \\ntest_column_mape: {test_column_mape} \\ntrain_column_mape: {train_column_mape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:05.075262100Z",
     "start_time": "2024-11-14T06:26:03.534740300Z"
    }
   },
   "id": "64f7bba46ecf7b6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### result record"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48faa35f7722880d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daijiaobu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAPE and loss\n",
    "\n",
    "2 layers without dropout, weight_decay, scheduler, lr=0.001\n",
    "for 1000:\n",
    "Epoch 250: train_loss: 0.0038760487, valid_loss: 0.6360880136\n",
    "test: 254.8839\n",
    "train: 46.2822   , 25.9250 if all columns standard\n",
    "\n",
    "for 10000:\n",
    "Epoch 400: train_loss: 0.0033661677, valid_loss: 0.2318614242\n",
    "test: 186.9136\n",
    "train: 54.1513\n",
    "\n",
    "layer3\n",
    "Epoch 250: train_loss: 0.0079129066, valid_loss: 0.2666302446\n",
    "test: 273.7292\n",
    "train: 64.5897\n",
    "\n",
    "4 layers \n",
    "Epoch 250: train_loss: 0.0151903867, valid_loss: 0.2816597309\n",
    "test: 374.9942\n",
    "train: 71.7741\n",
    "\"\"\"\n",
    "print('daijiaobu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:05.092299900Z",
     "start_time": "2024-11-14T06:26:05.078782400Z"
    }
   },
   "id": "1651d6ec16c5d8fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### predict once using random data from dataset\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2d5574b33cd6607"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one predict time: 0.0231 seconds\n",
      "\n",
      "choose from train:\n",
      "[0.6, 0.54, 0.33, -0.28]\n",
      "[0.5, 0.47, 0.24, -0.27]\n",
      "train_mape: 15.42165470123291\n",
      "\n",
      "choose from test:\n",
      "[0.18, 0.39, 0.42, -0.2]\n",
      "[0.14, 0.86, 0.7, 0.02]\n",
      "test_mape: 79.69793701171875\n"
     ]
    }
   ],
   "source": [
    "def print_tensor(a: torch.Tensor, is_pred=True):\n",
    "    list = a.squeeze().numpy().tolist()\n",
    "    c = [round(b, 2) for b in list]\n",
    "    if is_pred:\n",
    "        print(c)\n",
    "    else:\n",
    "        print(c[1:])\n",
    "    \n",
    "model.to('cpu')\n",
    "num_train = random.randint(0, X_train.shape[0])\n",
    "num_test = random.randint(0, X_test.shape[0])\n",
    "\n",
    "train_input = train_dataset[num_train][0].reshape(1, -1, 1)\n",
    "train_output = train_dataset[num_train][1].reshape(1, -1, 1)\n",
    "test_input = test_dataset[num_test][0].reshape(1, -1, 1)\n",
    "test_output = test_dataset[num_test][1].reshape(1, -1, 1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    train_pred = model(train_input, train_output)\n",
    "    end_time = time.time()\n",
    "    test_pred = model(test_input, test_output)\n",
    "\n",
    "consume_time = end_time - start_time\n",
    "print(f'one predict time: {consume_time:.4f} seconds\\n')\n",
    "print('choose from train:')\n",
    "print_tensor(train_output, False)\n",
    "print_tensor(train_pred)\n",
    "print(f'train_mape: {get_mape(train_output[:, 1:, :], train_pred)}\\n')\n",
    "\n",
    "print('choose from test:')\n",
    "print_tensor(test_output, False)\n",
    "print_tensor(test_pred)\n",
    "print(f'test_mape: {get_mape(test_output[:, 1:, :], test_pred)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:05.155925200Z",
     "start_time": "2024-11-14T06:26:05.098812800Z"
    }
   },
   "id": "d33f45a6d035c1be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6791735c50d65b4a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T06:26:05.157430500Z",
     "start_time": "2024-11-14T06:26:05.139406Z"
    }
   },
   "id": "afe5c15b9f79231b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
